{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import fasttext\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import SG_CORPUS, SG_FULL, CHECKPOINTS_DIR, PROBLEM_TEST\n",
    "from data import HatefulTweets, TextDataset\n",
    "from experiment import run_repeated_mlp, test_inference_time, calculate_memory_usage\n",
    "from nn import BinaryMLP\n",
    "from text_processing import get_fasttext_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "Global seed set to 2\n",
      "Global seed set to 3\n",
      "Global seed set to 4\n",
      "Global seed set to 5\n",
      "Global seed set to 6\n",
      "Global seed set to 7\n",
      "Global seed set to 8\n",
      "Global seed set to 9\n",
      "Global seed set to 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test/loss': '0.3520 ± 0.0164',\n",
       " 'test/f1': '0.5026 ± 0.0283',\n",
       " 'test/acc': '0.8948 ± 0.0058',\n",
       " 'test/precision': '0.6879 ± 0.0458',\n",
       " 'test/recall': '0.3970 ± 0.0292',\n",
       " 'train/loss': '0.1511 ± 0.0316',\n",
       " 'train/f1': '0.8504 ± 0.0278',\n",
       " 'train/acc': '0.9741 ± 0.0050',\n",
       " 'train/precision': '0.8369 ± 0.0359',\n",
       " 'train/recall': '0.8646 ± 0.0213',\n",
       " 'train_time': '23.5230 ± 2.0934'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_repeated_mlp(SG_CORPUS, name=\"mlp_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "Global seed set to 2\n",
      "Global seed set to 3\n",
      "Global seed set to 4\n",
      "Global seed set to 5\n",
      "Global seed set to 6\n",
      "Global seed set to 7\n",
      "Global seed set to 8\n",
      "Global seed set to 9\n",
      "Global seed set to 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test/loss': '0.3528 ± 0.0139',\n",
       " 'test/f1': '0.4802 ± 0.0343',\n",
       " 'test/acc': '0.8893 ± 0.0058',\n",
       " 'test/precision': '0.6491 ± 0.0430',\n",
       " 'test/recall': '0.3828 ± 0.0400',\n",
       " 'train/loss': '0.1464 ± 0.0315',\n",
       " 'train/f1': '0.8536 ± 0.0268',\n",
       " 'train/acc': '0.9747 ± 0.0050',\n",
       " 'train/precision': '0.8413 ± 0.0394',\n",
       " 'train/recall': '0.8667 ± 0.0163',\n",
       " 'train_time': '24.0255 ± 2.2360'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_repeated_mlp(SG_FULL, name=\"mlp_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = fasttext.load_model(str(SG_CORPUS))\n",
    "get_embeddings = partial(get_fasttext_embeddings, embeddings_model)\n",
    "\n",
    "dataset = TextDataset(PROBLEM_TEST, get_embeddings)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "checkpoint_file = CHECKPOINTS_DIR / \"mlp_corpus_1.ckpt\"\n",
    "model = BinaryMLP.load_from_checkpoint(\n",
    "    checkpoint_file,\n",
    "    emb_dim=300,\n",
    "    hidden_dims=[512, 256, 128, 64],\n",
    "    learning_rate=1e-4,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0004 ± 0.0002'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inference_time(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.261 MB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_memory_usage(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
